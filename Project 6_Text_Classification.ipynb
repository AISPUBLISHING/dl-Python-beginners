{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MyText_Classification.ipynb","version":"0.3.2","provenance":[{"file_id":"1ymnjLjJ5wyhUkq4uHIXRZaJ1qqqmt4m-","timestamp":1562509757014}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6LHuRL-gl8hC","colab_type":"text"},"source":["# **Deep Learning for Text and Sequences**"]},{"cell_type":"markdown","metadata":{"id":"8Fxad6G9l4Oi","colab_type":"text"},"source":["**What is NLP (Natural Language Processing)?**\n","\n","NLP falls under the domain of AI which involves working with natural languages to perform various tasks like **Language Translation**, **Speech Recoginition** (Translate spoken language into text by computers), **Spam Classification** and **many more**. "]},{"cell_type":"markdown","metadata":{"id":"bSMnLtnwnziA","colab_type":"text"},"source":["We have divided this notebook into two main sections:\n","\n","1. In this section we will give a brief introduction to basic NLP concepts using NLTK (Natual Language ToolKit)\n","\n","2. In this section we will go through RNN (Recurrent Neural Network) and build a model in Keras to perform semantic ananlysis"]},{"cell_type":"markdown","metadata":{"id":"jq6xfRrRpk1o","colab_type":"text"},"source":["### Part I: Introduction to basic NLP Concepts\n","\n","We will be covering following concepts in this part:\n","\n","1. Tokenization (Sentence and Word Tokenization)\n","2. Lemmatization and Stemming\n","3. Stop Words Removal\n","4. Regex\n"]},{"cell_type":"markdown","metadata":{"id":"QnN9lS_usGyb","colab_type":"text"},"source":["## Introduction to NLTK\n","\n","Before we jump into code lets first see what is NLTK. It is an open source platform for working on human language data in python. It provides easily usable interfaces to text handling and manipulation programs like tokenization, stemming, lemmatization and many more. It has variety of corpora availabel to work with. In this tutorial we will be taking advatange these libraries to demonstrate theoritical concepts in code."]},{"cell_type":"code","metadata":{"id":"RpzY3G1Is9vK","colab_type":"code","outputId":"e15cfdb7-0d8a-454b-c8f0-0d48f3e71ef1","executionInfo":{"status":"ok","timestamp":1562325665076,"user_tz":-300,"elapsed":8093,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#Lets install NLTK first\n","!pip install --user -U nltk\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: nltk in /root/.local/lib/python3.6/site-packages (3.4.4)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iQO1WwRAtJDu","colab_type":"code","outputId":"f03f8e6a-005e-4375-be12-51368bd2e455","executionInfo":{"status":"ok","timestamp":1562325666329,"user_tz":-300,"elapsed":9169,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":885}},"source":["#imports\n","import nltk\n","print ('nltk package imported successfully')\n","nltk.download('popular') #download some particular datasets and models"],"execution_count":0,"outputs":[{"output_type":"stream","text":["nltk package imported successfully\n"],"name":"stdout"},{"output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"P7QDypwSt6fD","colab_type":"text"},"source":["## Tokenization\n","\n","Tokenization is process of splitting string of language into sentences and words. This splitting is done by seperating tokens (sentences and words) on the basis of punctuations. For example, sentences are split using periods and words by using blank spaces. However it is a very simple way of defining of tokenization as this is not always true for other languages."]},{"cell_type":"code","metadata":{"id":"EeqfxMhLtm8n","colab_type":"code","outputId":"deb502b1-3622-4df0-b3e1-bfd984e10667","executionInfo":{"status":"ok","timestamp":1562325666335,"user_tz":-300,"elapsed":9029,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#Example for tokenization\n","paragraph = 'Representation learning has been a well known study in the computational intelligence space for years but its '\\\n","+ 'importance has taken off lately with the birth of deep learning. While conventional computational intelligence techniques '\\\n","+ 'such as classification often look at mathematically well-formed datasets, deep learning models look at data such as images '\\\n","+ 'that have not well-structured features. In that sense, representation learning is a an important feature of most deep learning architectures.'\n","\n","print ('Paragraph : ' , paragraph)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Paragraph :  Representation learning has been a well known study in the computational intelligence space for years but its importance has taken off lately with the birth of deep learning. While conventional computational intelligence techniques such as classification often look at mathematically well-formed datasets, deep learning models look at data such as images that have not well-structured features. In that sense, representation learning is a an important feature of most deep learning architectures.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W2_aK--RMoQu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAKOenCmwrxC","colab_type":"code","outputId":"1c2dabbc-c11c-48c0-d7bc-b9265ff52101","executionInfo":{"status":"ok","timestamp":1562325666349,"user_tz":-300,"elapsed":8893,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["#Tokenize the paragraph into sentences\n","sentences = nltk.sent_tokenize(paragraph)\n","\n","#Lets see the sentences\n","for sentence in enumerate (sentences):\n","  print (sentence)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(0, 'Representation learning has been a well known study in the computational intelligence space for years but its importance has taken off lately with the birth of deep learning.')\n","(1, 'While conventional computational intelligence techniques such as classification often look at mathematically well-formed datasets, deep learning models look at data such as images that have not well-structured features.')\n","(2, 'In that sense, representation learning is a an important feature of most deep learning architectures.')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k5cMMs9mJ1NI","colab_type":"text"},"source":["**sent_tokenize (text, language='english')**\n","1. Params - This built in nltk function takes two params: text - the string to be tokenized and language - language of the string to be tokenized which is by default set to English\n","2. Working - This function uses an unsupervised algorithm to create sentences considering a whole lot of factors like: words used at the start of the sentences, sentence boundaries, abberivation words, and collocations.\n","3. Return - returns a list of tokenized sentences from the text"]},{"cell_type":"code","metadata":{"id":"4sSNLdGMy0aG","colab_type":"code","outputId":"df496a89-56ea-4a60-f3c0-1ed2c450f52d","executionInfo":{"status":"ok","timestamp":1562325666352,"user_tz":-300,"elapsed":8765,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["#Now lets tokenize sentences into words\n","for sentence in sentences:\n","  words = nltk.word_tokenize(sentence) #calls the function to tokenize sentence into words\n","  print (len(words)) #lets just print length and not all the words to avoid clutter"],"execution_count":0,"outputs":[{"output_type":"stream","text":["29\n","30\n","17\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iIdZ61jwQ6eB","colab_type":"text"},"source":["**word_tokenize(text, language='english', preserve_line=False)**\n","1. Params - This built in nltk function takes three params: text - the string to be tokenized, language - language of the string to be tokenized which is by default set to English, and preserve_line - flag to tokenize the text into sentences before word tokenizing it. It is false by default.\n","2. Working - This function uses regular expressions to tokenize the text. Roughly, it standardizes the contractions, treats most punctuation token as seperators, and filter periods seperate that appear at the end of the sentencce\n","3. Return - returns a list of tokenized words from the text"]},{"cell_type":"markdown","metadata":{"id":"YgR1zQCfy5gZ","colab_type":"text"},"source":["## Lemmatization and Stemming\n","\n","Lemmatization and Stemming are used to reduce set of derived words to a common base word. \n","\n","For example: dog, dogs, dog's, dogs' should all be reduced to one common form **dog** \n","\n","Stemming does this by chopping off the beginning or end of words to remove affixes (prefixes and suffixes). driving is converted to drive\n","\n","Lemmatization does the similiar thing by reducing words to their dictionary form. For example words like best, excellent are changed to good as a standard, past, future, and continous forms of tenses are resolved to single standard form. "]},{"cell_type":"code","metadata":{"id":"FM5LZ3sG25Qq","colab_type":"code","outputId":"6d82aad7-adb6-42e1-8f34-0e28e39e768f","executionInfo":{"status":"ok","timestamp":1562325668516,"user_tz":-300,"elapsed":10758,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["#Lets see the code\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import wordnet #for providing part of speech to Lemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","print ('Stemmer for word drawing : ', stemmer.stem('drawing'))\n","print ('Stemmer for word drove : ', stemmer.stem('drove'))\n","       \n","print ('Lemmatizer for word drove : ', lemmatizer.lemmatize('drove', wordnet.VERB))\n","print ('Lemmatizer for word drove : ', lemmatizer.lemmatize('drawing', wordnet.VERB))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Stemmer for word drawing :  draw\n","Stemmer for word drove :  drove\n","Lemmatizer for word drove :  drive\n","Lemmatizer for word drove :  draw\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yYoEv1kUTQNd","colab_type":"text"},"source":["**stemmer.stem(token)**\n","\n","1- Params - This function takes one param: token - the token to be stemmed\n","\n","2- Working - This function performs common stemming functionality like removing prefixes and suffixes, converting feminine to masculine form, and convert plural form to its singular form\n","\n","3- Return: return the stemmed token"]},{"cell_type":"markdown","metadata":{"id":"cx4GIFMwUeRD","colab_type":"text"},"source":["**lemmatizer.lemmatize(word, pos)**\n","\n","1- Params - This function takes two params: word - the word to be converted to base form and pos - it tells about the part of speech to which the word belongs\n","\n","2- Working - This function performs a set of morphological operations to find the base word for the passed word in wordnet (one of the corpora available in nltk) database\n","\n","3- Return: return the lemmatized word"]},{"cell_type":"markdown","metadata":{"id":"dce_rSOa5Up4","colab_type":"text"},"source":["## Stop Words Removal\n","\n","Stop words are the words that most commonly used and do not add much of semantic meaning to the text therefore we filter out these words before applying algorithms to remove noise. Some common stop words include **we**, **a**, **you** "]},{"cell_type":"code","metadata":{"id":"2ghWO5gS6GZb","colab_type":"code","outputId":"46d0adff-8855-418f-a6c2-ed24f0772797","executionInfo":{"status":"ok","timestamp":1562325668521,"user_tz":-300,"elapsed":10621,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#lets see the collection of stopwords in nltk\n","from nltk.corpus import stopwords #import stopwords from nltk corpus\n","print(stopwords.words(\"english\")) #Stopwords in English Language"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2shKxBln6Xjj","colab_type":"code","outputId":"97ba95fe-f151-4708-d75f-ea2534c67004","executionInfo":{"status":"ok","timestamp":1562325668524,"user_tz":-300,"elapsed":10485,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#lets see how our paragraph looks like without stopwords\n","stop_words = stopwords.words(\"english\")\n","words = nltk.word_tokenize(paragraph)\n","for word in words:\n","  if not word in stop_words:\n","    print (word, sep=' ' , end=' ')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Representation learning well known study computational intelligence space years importance taken lately birth deep learning . While conventional computational intelligence techniques classification often look mathematically well-formed datasets , deep learning models look data images well-structured features . In sense , representation learning important feature deep learning architectures . "],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qEcHvYsO8Olu","colab_type":"text"},"source":["## Regex\n","\n","Regex (Regular Expression) is a sequence of characters that defines a search pattern or in simpler words it is a method for a programmer to tell the program what patterns to look for and what actions to take once those patterns are found in the text. Regex is a very widely used to technique to cleanse data with particular type of noise. "]},{"cell_type":"code","metadata":{"id":"qmeNhFH590vL","colab_type":"code","outputId":"f7ff1d59-97a9-4c45-9396-cb75945cc095","executionInfo":{"status":"ok","timestamp":1562325668529,"user_tz":-300,"elapsed":10415,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#lets see how to apply a simple regex in python\n","import re\n","\n","result = re.sub(\"\\ \", \"_\", paragraph) #replace blank space with '_' in the paragraph \n","print(result) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Representation_learning_has_been_a_well_known_study_in_the_computational_intelligence_space_for_years_but_its_importance_has_taken_off_lately_with_the_birth_of_deep_learning._While_conventional_computational_intelligence_techniques_such_as_classification_often_look_at_mathematically_well-formed_datasets,_deep_learning_models_look_at_data_such_as_images_that_have_not_well-structured_features._In_that_sense,_representation_learning_is_a_an_important_feature_of_most_deep_learning_architectures.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JdXv4qgI2E8K","colab_type":"text"},"source":["**re.sub (pattern, repl, string, count=0)**\n","\n","1- Params - This function takes four params: pattern - the pattern that is to replace, repl - replacement for the pattern, string - string that is to replaced and count for the maximum number of occurences are to replaced \n","\n","2- Working: Returns a updated string and if no matching pattern is found then the unchanged string\n","\n","3- Updated string after replacements (if pattern is found)"]},{"cell_type":"markdown","metadata":{"id":"e2ftvg1p_8ym","colab_type":"text"},"source":["### Part II: Building RNN to Classify Sentimets\n","\n","In this section we will go through a basic definition of RNN (Recurrent neural network), LSTM (Long Short-term Memory) Network and build a classifier with LSTM layer in Keras to classify sentiments.\n","\n","**So, What is a RNN?**\n","\n","RNNs are artificial neural networks that unlike other ANNs (Artificial Neural Networks) remember history from previous steps. They contain a hidden layer which is responsible for maintaining this history. These networks are particularly useful when dealing with time series data, text sequences or sound data.\n","\n","However, RNNs are not particularly useful in solving problems that require remembering long term sequences due to **vanishing gradients** problem. A variation of RNNs known as LSTM is used which contains a memory cell along with a gating mechanism to decide entry and exit for memory with the cell."]},{"cell_type":"markdown","metadata":{"id":"yZsJ51WIESD7","colab_type":"text"},"source":["## Word Embeddings\n","\n","Before we jump into code lets first discuss about word embeddings. Word embeddings is a way of representing text such that the words with similiar contexts have smaller distance as compared to words with different meanings. For example, words like good, best will lie closer to each other in the coodinate system. \n","\n","Two famous word embedding models are:\n","\n","1. Word2Vec\n","2. Glove"]},{"cell_type":"code","metadata":{"id":"s2ydCFbCKnju","colab_type":"code","outputId":"8e9b4334-6dc5-42ac-cd91-de2b3ab6e897","executionInfo":{"status":"ok","timestamp":1562391041825,"user_tz":-300,"elapsed":2379,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np #for LA operations\n","import pandas as pd #for data manipulation\n","import os #for os related functions\n","\n","#Imports from keras and tokenizer API \n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"f109yqDWxF-q","colab_type":"text"},"source":["## Model Building and Training \n","\n","Now we are all set to build our model. We will be building models one with an LSTM layer and another with Conv1D layer.  We will be using preloaded dataset for LSTM network whereas for CNN we will be using manually downloaded dataset. Both the datasets are same but we wanted to give an idea how can we do it in two different ways."]},{"cell_type":"markdown","metadata":{"id":"W59v5tJ4XMMy","colab_type":"text"},"source":["## Dataset loading\n","\n","First we will be using keras preloaded dataset of **Large movie review dataset** to give you an overview of how to use built in datasets in Keras\n","\n","**Dataset** - Large Movie Dataset provides 25000 movie reviews for training and same number of reviews for the training. Where each review has either a positive (**representated as 1**) or negative (**representated as 0**) sentiment.\n","\n","**max_words** -  shows the maximum length of a review if any review is greater than that we will turncate it and zero pad the smaller ones\n","\n","**top_words** - defines how many top words to load from the dataset and zero out the words other than those top 5000"]},{"cell_type":"code","metadata":{"id":"HyR7_VzdXKCu","colab_type":"code","colab":{}},"source":["from keras.datasets import imdb #we will be using preloaded dataset for LSTM and manually prepared for Conv\n","\n","#Lets first build LSTM with using imdb API for the dataset\n","# fix random seed for reproducibility\n","seed = 7\n","np.random.seed(seed)\n","\n","np_load_old = np.load\n","\n","# modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n","\n","\n","max_words = 500\n","top_words = 5000\n","EMBEDDING_DIM = 100\n","\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","\n","np.load = np_load_old #revert back to default parameters \n","#it is a way around to deal with different versions of libraries"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtmL2aOuYzd4","colab_type":"text"},"source":["**Padding**\n","\n","Pad the sequences (reviews) that are shorter than the max_words"]},{"cell_type":"code","metadata":{"id":"_zc22ZN8Y0af","colab_type":"code","colab":{}},"source":["X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cK2jmkdV4Ptd","colab_type":"text"},"source":["# Activation Functions in NNs \n","\n","Activation functions are an essential component of a Neural Network and help in learning non-linear mappings that are otherwise impossible to learn with just linear functions. There are many activation functions that can be used in NN layers. Each one has its pros and cons in this tutorial we going to a very popular activation function - **RELU (rectified linear unit)** whcih clips the output of neuron to zero in case its output is less than zero. It is mathematically defined as **max(0,x) **"]},{"cell_type":"markdown","metadata":{"id":"Ja_O4QhM2J4X","colab_type":"text"},"source":["## Model Building"]},{"cell_type":"code","metadata":{"id":"e41CobUOWG2u","colab_type":"code","outputId":"1b6c89c2-f160-464c-9da4-570b08302ab2","executionInfo":{"status":"ok","timestamp":1562325677313,"user_tz":-300,"elapsed":19006,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["print ('Building The Model')\n","model = Sequential() #Define the model type which is sequential in our case\n","model.add(Embedding(top_words, EMBEDDING_DIM, input_length=max_words)) #this is the syantax to add a layer and embedding is the layer\n","model.add(LSTM(units = 32, dropout = 0.2, recurrent_dropout = 0.2)) #LSTM - Recurrent Layer)\n","model.add(Dense(250, activation='relu')) #add a Dense layer with relu activation function \n","model.add(Dense(1, activation='sigmoid')) #add output layer\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print ('Model Built Successfully')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0705 11:21:16.396420 139853253687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0705 11:21:16.416953 139853253687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0705 11:21:16.420805 139853253687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0705 11:21:16.504194 139853253687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0705 11:21:16.515108 139853253687168 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["Building The Model\n"],"name":"stdout"},{"output_type":"stream","text":["W0705 11:21:16.842748 139853253687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0705 11:21:16.866696 139853253687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0705 11:21:16.873373 139853253687168 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Model Built Successfully\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ufRJf_qQ2OMV","colab_type":"text"},"source":["## Model Summary"]},{"cell_type":"code","metadata":{"id":"kreqFOZfZJi3","colab_type":"code","outputId":"0b598ab1-7362-4837-c771-26b5e3268ee5","executionInfo":{"status":"ok","timestamp":1562325677317,"user_tz":-300,"elapsed":18936,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 500, 100)          500000    \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 32)                17024     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 250)               8250      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 251       \n","=================================================================\n","Total params: 525,525\n","Trainable params: 525,525\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U6oIao1q2Vy1","colab_type":"text"},"source":["## Model Training\n","\n","In this step we will be training our model with following configurations\n","\n","1- Epochs 20 - which means that our model will look at each example in our dataset 20 times.\n","2- Validation Split 0.3 - which means 30% of our training data will be used for validating the model\n","3- Batch size 128 -  which means there are going to be 128 examples used in each iteration from which our model will learn in a single step"]},{"cell_type":"code","metadata":{"id":"gLVphQl_ZOfc","colab_type":"code","outputId":"1b39ef93-a800-4c80-fc99-cd3fb8e6b915","executionInfo":{"status":"ok","timestamp":1562327878672,"user_tz":-300,"elapsed":2220210,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":764}},"source":["print ('Start Training')\n","model.fit(X_train, y_train, validation_split=0.3, epochs=20, batch_size=128)\n","print ('Training Finished')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Start Training\n","Train on 17500 samples, validate on 7500 samples\n","Epoch 1/20\n","17500/17500 [==============================] - 111s 6ms/step - loss: 0.5225 - acc: 0.7366 - val_loss: 0.4036 - val_acc: 0.8173\n","Epoch 2/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.3794 - acc: 0.8374 - val_loss: 0.4830 - val_acc: 0.7801\n","Epoch 3/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.3234 - acc: 0.8665 - val_loss: 0.3859 - val_acc: 0.8357\n","Epoch 4/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.3052 - acc: 0.8750 - val_loss: 0.3904 - val_acc: 0.8324\n","Epoch 5/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.2811 - acc: 0.8885 - val_loss: 0.4196 - val_acc: 0.8088\n","Epoch 6/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.2740 - acc: 0.8882 - val_loss: 0.4271 - val_acc: 0.8177\n","Epoch 7/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.2402 - acc: 0.9049 - val_loss: 0.4119 - val_acc: 0.8380\n","Epoch 8/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.2288 - acc: 0.9101 - val_loss: 0.4184 - val_acc: 0.8284\n","Epoch 9/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1947 - acc: 0.9249 - val_loss: 0.4874 - val_acc: 0.8245\n","Epoch 10/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1747 - acc: 0.9321 - val_loss: 0.4757 - val_acc: 0.8141\n","Epoch 11/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1505 - acc: 0.9429 - val_loss: 0.5348 - val_acc: 0.8036\n","Epoch 12/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.2145 - acc: 0.9117 - val_loss: 0.5135 - val_acc: 0.8091\n","Epoch 13/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1662 - acc: 0.9346 - val_loss: 0.5398 - val_acc: 0.8217\n","Epoch 14/20\n","17500/17500 [==============================] - 109s 6ms/step - loss: 0.1411 - acc: 0.9449 - val_loss: 0.6341 - val_acc: 0.8143\n","Epoch 15/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1212 - acc: 0.9546 - val_loss: 0.6286 - val_acc: 0.8032\n","Epoch 16/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1095 - acc: 0.9593 - val_loss: 0.6380 - val_acc: 0.8128\n","Epoch 17/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.0990 - acc: 0.9626 - val_loss: 0.7176 - val_acc: 0.7961\n","Epoch 18/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1024 - acc: 0.9610 - val_loss: 0.7101 - val_acc: 0.7985\n","Epoch 19/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.1021 - acc: 0.9628 - val_loss: 0.7319 - val_acc: 0.8111\n","Epoch 20/20\n","17500/17500 [==============================] - 110s 6ms/step - loss: 0.0824 - acc: 0.9707 - val_loss: 0.7655 - val_acc: 0.7991\n","Training Finished\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lqbEFrac2az4","colab_type":"text"},"source":["## Model Evaluation"]},{"cell_type":"code","metadata":{"id":"mRC6hS1SZUNt","colab_type":"code","outputId":"01b0518b-cb8e-4153-e470-aed903c59cf4","executionInfo":{"status":"ok","timestamp":1562327921059,"user_tz":-300,"elapsed":2262522,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print ('Evaluating the model on the Test Set')\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Evaluating the model on the Test Set\n","Accuracy: 80.22%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vKMzM-7iZkls","colab_type":"text"},"source":["## Using External Dataset\n","\n","We already saw how to use Keras preloaded dataset. Now we are going to download and format an external dataset for our CNN."]},{"cell_type":"code","metadata":{"id":"_F2sjjmlJUyA","colab_type":"code","outputId":"c14183be-3146-483e-e23d-ae40cb7dbeae","executionInfo":{"status":"ok","timestamp":1562391065239,"user_tz":-300,"elapsed":15289,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["#Download Large Movie Review Dataset and unzip it \n","!wget ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar xzf aclImdb_v1.tar.gz"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-07-06 05:30:51--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  21.7MB/s    in 3.7s    \n","\n","2019-07-06 05:30:55 (21.7 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5aWuWCuIaJP0","colab_type":"text"},"source":["## Prepare the dataset\n","\n","In this step we will be reading the files and appending them to a single pandas dataframe so that we can use it for training and testing our model"]},{"cell_type":"code","metadata":{"id":"OlkyskfvPqdb","colab_type":"code","colab":{}},"source":["#lets format the dataset to our purpose\n","folder = 'aclImdb' #where dataset is extracted\n","labels = {'pos': 1, 'neg': 0} #two classes\n","splits = ('test', 'train') #two splits in dataset\n","data_frame = pd.DataFrame() #create a pandas dataframe\n","\n","for split in splits:    \n","    for label in ('pos', 'neg'):\n","        path = os.path.join(folder, split, label) #join using path.join as it is independent of os\n","        for file in os.listdir (path) : #list all the directories in the path\n","            with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n","                txt = infile.read()\n","            data_frame = data_frame.append([[txt, labels[label]]],ignore_index=True)\n","\n","data_frame.columns = ['review', 'sentiment']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gwif7arahl3","colab_type":"text"},"source":["## Analyze the dataset\n","\n","We will be looking at a few records from our dataset, total number of records in our dataset and class distribution (number of positive and negative sentiments) in our dataset"]},{"cell_type":"code","metadata":{"id":"7Gy4gt4_SfbR","colab_type":"code","outputId":"ce5e0d59-8b3c-446e-fb1a-99046e7b5e97","executionInfo":{"status":"ok","timestamp":1562328057114,"user_tz":-300,"elapsed":2398422,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["#lets analyze the transformed dataset\n","data_frame.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From a perspective that it is possible to make...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I would say to the foreign people who have see...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Less Than Zero could have been the 80s movie t...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I just watched this movie on it's premier nigh...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I had been interested in this film for a long ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  From a perspective that it is possible to make...          1\n","1  I would say to the foreign people who have see...          1\n","2  Less Than Zero could have been the 80s movie t...          1\n","3  I just watched this movie on it's premier nigh...          1\n","4  I had been interested in this film for a long ...          1"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"uUdWwcMiS-Qp","colab_type":"code","outputId":"7d284562-4eb5-420b-fbac-de5e5901f335","executionInfo":{"status":"ok","timestamp":1562328057118,"user_tz":-300,"elapsed":2398353,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print ('Total Records = ' , len(data_frame))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total Records =  50000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rZUbEwZGS-F7","colab_type":"code","outputId":"0522ce5b-d4f3-4df4-e9c5-535131d71c42","executionInfo":{"status":"ok","timestamp":1562392075803,"user_tz":-300,"elapsed":849,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":298}},"source":["#to plot the bar chart\n","import matplotlib.pyplot as plt\n","\n","print ('Class Distribution')\n","\n","sentiments = ('Negative', 'Positive')\n","y_pos = np.arange(len(sentiments))\n","counts = [data_frame.sentiment.value_counts()[0], data_frame.sentiment.value_counts()[1]] \n","\n","\n","plt.bar(y_pos, counts, align='center', alpha=0.5) \n","plt.xticks(y_pos, sentiments) \n","plt.ylabel('Counts')\n","plt.title('Class Distribution')\n","\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Class Distribution\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZUAAAEICAYAAACXo2mmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmdMVQkSZ0xVCRJnTFUJEmd+X9Fs8rxMSnxEAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"dqz2vd0GVK1L","colab_type":"code","colab":{}},"source":["#Lets prepare the dataset for Conv Network without using imdb API from Keras\n","x_train_manual = data_frame.loc[:24999, 'review'].values\n","y_train_manual = data_frame.loc[:24999, 'sentiment'].values\n","x_test_manual = data_frame.loc[25000:, 'review'].values\n","y_test_manual = data_frame.loc[25000:, 'sentiment'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0Y58n5gVKpa","colab_type":"code","outputId":"77dd522e-625d-4715-8e7e-1db902ddb1ed","executionInfo":{"status":"ok","timestamp":1562328057138,"user_tz":-300,"elapsed":2398215,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#Reconfirm class distribution in train and test sets after split\n","print ('Distribution in train set ', np.unique(y_train_manual, return_counts= True))\n","print ('Distribution in test set ', np.unique(y_test_manual, return_counts= True))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Distribution in train set  (array([0, 1]), array([12500, 12500]))\n","Distribution in test set  (array([0, 1]), array([12500, 12500]))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hL7Mle09xcT-","colab_type":"text"},"source":["## Dataset Formating\n","\n","In this step we are going to convert our text sequence data into number sequences\n","\n","1. Initalize Tokenizer Object with top_words (as explained above to use only most frequent words (top_words) and make the rest zero) and fit on our training data\n","2. Convert the text sequences to number sequences by function texts_to_sequences and pass the text data\n","3. Pad the sequences that are less than the maxlen (max_words) by padding them"]},{"cell_type":"code","metadata":{"id":"2qHglgONCTFu","colab_type":"code","colab":{}},"source":["tok_train = Tokenizer(num_words=top_words)\n","tok_train.fit_on_texts(x_train_manual)\n","\n","x_train_manual = tok_train.texts_to_sequences(x_train_manual)\n","x_test_manual = tok_train.texts_to_sequences(x_test_manual)\n","\n","x_train_pad = sequence.pad_sequences(x_train_manual,maxlen=max_words)\n","x_test_pad = sequence.pad_sequences(x_test_manual,maxlen=max_words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcHh4AjE2D2h","colab_type":"text"},"source":["## Model Building"]},{"cell_type":"code","metadata":{"id":"hxIgGs5MqML4","colab_type":"code","outputId":"c602c62f-a43e-48bf-ee2f-ff4a288ab852","executionInfo":{"status":"ok","timestamp":1562328072900,"user_tz":-300,"elapsed":2413893,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["#import conv and max_pool layer for the CNN model\n","from keras.layers import Conv1D, MaxPool1D\n","\n","#Now We will build the Convolutional Model\n","print ('Building the CNN model')\n","model_conv = Sequential()\n","model_conv.add(Embedding(top_words, EMBEDDING_DIM, input_length=max_words))\n","model_conv.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n","model_conv.add(MaxPool1D(pool_size=2)) #pooling layer to reduce dimension\n","model_conv.add(Flatten()) #flatten the output from pool before passing to dense layer\n","model_conv.add(Dense(36, activation='relu'))\n","model_conv.add(Dense(1, activation='sigmoid'))\n","model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print ('CNN model build successfully')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0705 12:01:12.219458 139853253687168 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Building the CNN model\n","CNN model build successfully\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bsi8Qcpr2gZt","colab_type":"text"},"source":["## Model Summary"]},{"cell_type":"code","metadata":{"id":"SGbO-4aDa9eG","colab_type":"code","outputId":"352325be-7659-4abf-9e3a-23a40b5f09bd","executionInfo":{"status":"ok","timestamp":1562328072905,"user_tz":-300,"elapsed":2413828,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["print(model_conv.summary())\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 500, 100)          500000    \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 493, 32)           25632     \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 246, 32)           0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 7872)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 36)                283428    \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 37        \n","=================================================================\n","Total params: 809,097\n","Trainable params: 809,097\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8Zd3c0jb2ngP","colab_type":"text"},"source":["## Model Training"]},{"cell_type":"code","metadata":{"id":"JJqtkdTObATC","colab_type":"code","outputId":"98a57180-23b5-483b-d4af-3180910324f5","executionInfo":{"status":"ok","timestamp":1562328653764,"user_tz":-300,"elapsed":174921,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":416}},"source":["print ('Start Model Training')\n","model_conv.fit(x_train_pad, y_train_manual, validation_split=0.3, epochs=10, batch_size=128)\n","print ('Model Training finished')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Start Model Training\n","Train on 17500 samples, validate on 7500 samples\n","Epoch 1/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.4743 - acc: 0.7766 - val_loss: 0.8672 - val_acc: 0.7063\n","Epoch 2/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.3034 - acc: 0.9084 - val_loss: 1.0008 - val_acc: 0.6851\n","Epoch 3/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.2419 - acc: 0.9379 - val_loss: 0.9103 - val_acc: 0.7919\n","Epoch 4/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.1954 - acc: 0.9594 - val_loss: 1.0943 - val_acc: 0.7759\n","Epoch 5/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.1629 - acc: 0.9746 - val_loss: 1.1150 - val_acc: 0.8005\n","Epoch 6/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.1410 - acc: 0.9819 - val_loss: 1.3341 - val_acc: 0.7672\n","Epoch 7/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.1253 - acc: 0.9857 - val_loss: 1.1961 - val_acc: 0.8051\n","Epoch 8/10\n","17500/17500 [==============================] - 59s 3ms/step - loss: 0.1129 - acc: 0.9877 - val_loss: 1.5034 - val_acc: 0.7697\n","Epoch 9/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.1032 - acc: 0.9887 - val_loss: 1.5090 - val_acc: 0.7749\n","Epoch 10/10\n","17500/17500 [==============================] - 58s 3ms/step - loss: 0.0953 - acc: 0.9894 - val_loss: 1.6581 - val_acc: 0.7616\n","Model Training finished\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TB5KZS0L2sg9","colab_type":"text"},"source":["## Model Evaluation"]},{"cell_type":"code","metadata":{"id":"oEqUfIoHbK8K","colab_type":"code","outputId":"d4bc9c80-c438-4c44-b7ea-ee7a6c6498d5","executionInfo":{"status":"ok","timestamp":1562328670770,"user_tz":-300,"elapsed":17016,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Final evaluation of the model\n","print ('Evaluating the model')\n","scores = model_conv.evaluate(x_test_pad, y_test_manual, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Evaluating the model\n","Accuracy: 84.51%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G_m8Ag7FbTcY","colab_type":"text"},"source":["## Testing Samples on the CNN model\n","\n","Lets test our model on same reviews\n","\n","The resulting probablities positive reviews tend to be closer to 1 as it is the positive class "]},{"cell_type":"code","metadata":{"id":"5u645zionvzj","colab_type":"code","outputId":"04c2df8f-75a6-4d19-d632-7fd4ff312989","executionInfo":{"status":"ok","timestamp":1562329488162,"user_tz":-300,"elapsed":888,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["sample1 = \"awesome, must watch movie will difinetly watch again if I get the chance\"\n","sample2 = \"movie was really good, really liked the acting as well\"\n","sample3 = \"movie was not that good at most OK but I did like the acting\"\n","sample4 = \"movie was so horrible, very bad acting as well how can people watch this crap\"\n","sample5 = \"i have not seen a movie worse than this one\"\n","\n","test_sample = [sample1, sample2, sample3, sample4, sample5] #make a list of lists\n","test_sample_tok =  tok_train.texts_to_sequences(test_sample) #make sequences just as in done earlier steps\n","test_sample_pad =  sequence.pad_sequences (test_sample_tok, maxlen=max_words) #pad the sequences as done in earlier steps\n","\n","print (model_conv.predict(test_sample_pad))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.9980488 ]\n"," [0.9915957 ]\n"," [0.28260344]\n"," [0.23432045]\n"," [0.23432045]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EL6BAgwPg7M2","colab_type":"text"},"source":["## Testing Samples on the LSTM model\n","\n","This is a bit different from the above code because we have to use keras tokenizer as we did not build the tokenizer for the preloaded dataset\n"]},{"cell_type":"code","metadata":{"id":"Be-Egkb4dMgf","colab_type":"code","outputId":"28cd3e3f-0452-4c68-fc95-7d9d465ea8a8","executionInfo":{"status":"ok","timestamp":1562330752499,"user_tz":-300,"elapsed":3221,"user":{"displayName":"Nilesh Kumar","photoUrl":"","userId":"13369125912005154422"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["sample1 = \"awesome, must watch movie will difinetly watch again\"\n","sample2 = \"movie was awesome\"\n","sample3 = \"movie was not that good at most OK but I did like the acting\"\n","sample4 = \"movie was so horrible, very bad acting as well how can people watch this crap\"\n","sample5 = \"bad indeed\"\n","\n","test_sample = [sample1, sample2, sample3, sample4, sample5]\n","\n","#tk = Tokenizer(nb_words=top_words)\n","#tk.fit_on_texts(test_sample)\n","\n","sequences = tok_train.texts_to_sequences(test_sample)\n","padded_sequences = sequence.pad_sequences(sequences, maxlen=max_words)\n","\n","prediction = model.predict(padded_sequences)\n","print(prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.9996575 ]\n"," [0.99932814]\n"," [0.7962845 ]\n"," [0.11646644]\n"," [0.0058602 ]]\n"],"name":"stdout"}]}]}